{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae164f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f9bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Production runner with Historical vs Closed-Loop modes.\n",
    "\n",
    "What it does (idempotent):\n",
    "  1) Ensures ML predictions for the 6-month lookback + today's stamps → CSV cache\n",
    "  2) Ensures GP residual rows for the 6-month window → Pickle cache\n",
    "  3) Fits GP on the 6-month slice\n",
    "  4) (Per stamp) Auto-tunes per-product α by slope fidelity (rc0-injected), saves fidelity\n",
    "  5) Runs multi-knob RCOT optimizer (with anchored objective), prints/saves RCOT moves, price audit\n",
    "  6) Saves curves (overrides-only α, rc0-injected) + per-geometry fidelity audit\n",
    "  7) At the end: builds a counterfactual schedule and simulates corrected yields + margin\n",
    "     (consistent end-of-run check; for online closed-loop the state is updated in-loop)\n",
    "\n",
    "Supports modes:\n",
    "  - historical\n",
    "  - closed_loop: apply recommended RCOTs on a stateful copy of X and hold until next decision.\n",
    "    Keeps caches separate via a cache_tag to avoid polluting historical ones.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Sequence, List, Optional, Tuple, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.ml_predictor import MLPredictor, MLPredictorConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"X does not have valid feature names, but GaussianProcessRegressor was fitted with feature names\",\n",
    "    category=UserWarning,\n",
    "    module=\"sklearn\"\n",
    ")\n",
    "\n",
    "# ---- project imports ----\n",
    "from importlib import reload\n",
    "import src.gp_residuals as gpmod\n",
    "import src.optimizer    as opt\n",
    "\n",
    "\n",
    "# =================== CONFIG (static) ===================\n",
    "LOOKBACK_6M    = pd.Timedelta(days=180)\n",
    "MIN_TR_ROWS    = 180\n",
    "GP_JOBS        = 8\n",
    "\n",
    "START          = pd.Timestamp('2025-01-01')\n",
    "END            = pd.Timestamp('2025-05-19')\n",
    "\n",
    "OUT_DIR        = Path(\"prod_out\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Base OUT dir; mode-specific subdirs will be derived\n",
    "OUT_DIR_BASE   = Path(\"prod_out\")\n",
    "OUT_DIR_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# RCOT bounds (single source of truth)\n",
    "RC_BOUNDS = {\n",
    "    'LF_NAPH':     (830.0, 853.0),\n",
    "    'GF_GAS':      (860.0, 890.0),\n",
    "    'GF_HYB_NAPH': (830.0, 853.0),\n",
    "}\n",
    "\n",
    "# canonical products\n",
    "PRODS_CANON = (\"Ethylene\",\"Propylene\",\"Mixed C4\",\"RPG\",\"Hydrogen\",\"Tail Gas\")\n",
    "# canonical internal keys for corrected_yields_for_row\n",
    "PRODS_INTERNAL = ['Ethylene','Propylene','MixedC4','RPG','Ethane','Propane','Hydrogen','Tail_Gas']\n",
    "TARGET_COLS = [f'{p}_prod_t+1' for p in PRODS_INTERNAL]\n",
    "\n",
    "# =================== HELPERS ===================\n",
    "def _ts_per_day(index: pd.DatetimeIndex, day: pd.Timestamp) -> List[pd.Timestamp]:\n",
    "    d0 = pd.Timestamp(day).normalize()\n",
    "    d1 = d0 + pd.Timedelta(days=1)\n",
    "    idx = index[(index >= d0) & (index < d1)]\n",
    "    return list(idx)\n",
    "\n",
    "def _safe_merge_csv(path: Path, df_new: pd.DataFrame, key: str | list[str] = 'timestamp') -> pd.DataFrame:\n",
    "    key_cols = [key] if isinstance(key, str) else list(key)\n",
    "    if path.exists():\n",
    "        old = pd.read_csv(path, parse_dates=[k for k in key_cols if 'time' in k.lower()])\n",
    "        old = old.set_index(key_cols).sort_index()\n",
    "    else:\n",
    "        if len(key_cols) == 1:\n",
    "            k = key_cols[0]\n",
    "            idx = (pd.DatetimeIndex([], name=k) if 'time' in k.lower() else pd.Index([], name=k))\n",
    "            old = pd.DataFrame(index=idx)\n",
    "        else:\n",
    "            idx = pd.MultiIndex.from_arrays([[] for _ in key_cols], names=key_cols)\n",
    "            old = pd.DataFrame(index=idx)\n",
    "\n",
    "    new = df_new.copy()\n",
    "    for k in key_cols:\n",
    "        if k in new.columns and 'time' in k.lower():\n",
    "            new[k] = pd.to_datetime(new[k], errors='coerce')\n",
    "    new = new.set_index(key_cols).sort_index()\n",
    "\n",
    "    out = pd.concat([old[~old.index.isin(new.index)], new], axis=0).sort_index()\n",
    "    out_reset = out.reset_index()\n",
    "    out_reset.to_csv(path, index=False)\n",
    "    return out_reset\n",
    "\n",
    "def _safe_merge_pickle(path: Path, df_new: pd.DataFrame, min_index: pd.Timestamp | None = None) -> pd.DataFrame:\n",
    "    if path.exists():\n",
    "        old = pd.read_pickle(path)\n",
    "        old.index = pd.to_datetime(old.index)\n",
    "    else:\n",
    "        old = pd.DataFrame()\n",
    "\n",
    "    if df_new is not None and len(df_new):\n",
    "        df_new = df_new.copy()\n",
    "        df_new.index = pd.to_datetime(df_new.index)\n",
    "        out = pd.concat([old, df_new], axis=0)\n",
    "    else:\n",
    "        out = old\n",
    "\n",
    "    if min_index is not None and not out.empty:\n",
    "        out = out.loc[out.index >= pd.Timestamp(min_index)]\n",
    "\n",
    "    out = out[~out.index.duplicated(keep='last')].sort_index()\n",
    "    out.to_pickle(path)\n",
    "    return out\n",
    "\n",
    "def realized_margin_from_Y(ts: pd.Timestamp, x_row: pd.Series, Y_12h: pd.DataFrame, margin_fn) -> float:\n",
    "    ydict = {}\n",
    "    for p in PRODS_INTERNAL:\n",
    "        col = f\"{p}_prod_t+1\"\n",
    "        ydict[col] = float(Y_12h.at[ts, col]) if (ts in Y_12h.index and col in Y_12h.columns) else 0.0\n",
    "    return float(margin_fn(ts, x_row, ydict))\n",
    "\n",
    "def _geometry_label_for_row(row: pd.Series) -> str:\n",
    "    n = sum(row.get(f'Naphtha_chamber{i}', 0.0) for i in range(1,7))\n",
    "    g = sum(row.get(f'Gas Feed_chamber{i}', 0.0) for i in (4,5,6))\n",
    "    if n>0 and g>0: return 'GF_HYB_NAPH'\n",
    "    if n>0:         return 'LF_NAPH'\n",
    "    if g>0:         return 'GF_GAS'\n",
    "    return 'NONE'\n",
    "\n",
    "def _norm_geom(g: str) -> str:\n",
    "    return str(g).strip().replace(\" \",\"_\").upper()\n",
    "\n",
    "def _bounds_for_geoms() -> dict[str, tuple[float,float]]:\n",
    "    return {'LF_NAPH':(800.0,895.0), 'GF_GAS':(820.0,910.0), 'GF_HYB_NAPH':(800.0,895.0)}\n",
    "\n",
    "def _rc_grid_for(ts_row: pd.Series, geom: str, lo: float, hi: float, points: int=15) -> np.ndarray:\n",
    "    rc0 = gpmod.rc0_guess_for_geom(ts_row, geom, fallback_rc=None)\n",
    "    base = np.linspace(lo, hi, points)\n",
    "    return np.unique(np.r_[base, rc0]) if rc0 is not None else base\n",
    "\n",
    "# =================== ML CACHE ADAPTER ===================\n",
    "class MLCacheAdapter:\n",
    "    \"\"\"Adapter for cached ML predictions; provides transform() (no-op) and predict_row().\"\"\"\n",
    "    def __init__(self, pred_df: pd.DataFrame):\n",
    "        self.pred_df = pred_df\n",
    "\n",
    "    def transform(self, X_12h: pd.DataFrame, Y_12h: pd.DataFrame):\n",
    "        # interface compliance; not used by GP table\n",
    "        return X_12h\n",
    "\n",
    "    def predict_row(self, row_like, **kwargs):\n",
    "        ts = getattr(row_like, 'name', None)\n",
    "        if ts is None or ts not in self.pred_df.index:\n",
    "            return {}\n",
    "        s = self.pred_df.loc[ts]\n",
    "        return {c: float(s.get(c, np.nan)) for c in self.pred_df.columns}\n",
    "\n",
    "# =================== FIDELITY (slope-only gate) ===================\n",
    "def _robust_slope_metrics(curve: pd.DataFrame, prod: str) -> tuple[float,float]:\n",
    "    if curve is None or curve.empty or len(curve) < 3:\n",
    "        return (np.nan, 0.0)\n",
    "    s = curve.get(f'{prod}_SRTO_tph'); c = curve.get(f'{prod}_CORR_tph')\n",
    "    if s is None or c is None: return (np.nan, 0.0)\n",
    "    s = s.to_numpy(float); c = c.to_numpy(float)\n",
    "    ds, dc = np.diff(s), np.diff(c)\n",
    "    ds_f = ds[np.isfinite(ds)]\n",
    "    if ds_f.size == 0: return (np.nan, 0.0)\n",
    "    eps  = 0.01*(np.nanpercentile(np.abs(ds_f),95) + 1e-12)\n",
    "    mask = np.abs(ds) >= eps\n",
    "    if mask.sum() < 3 or not np.isfinite(dc[mask]).all(): return (np.nan, float(mask.mean()))\n",
    "    return (float(np.corrcoef(ds[mask], dc[mask])[0,1]), float(mask.mean()))\n",
    "\n",
    "def _rcot_setter_single_knob(knob: str):\n",
    "    def _setter(row_like: pd.Series, rc: float) -> pd.Series:\n",
    "        row_like[knob] = float(rc)\n",
    "        return row_like\n",
    "    return _setter\n",
    "\n",
    "def _local_rc_grid(rc0: float, lo: float, hi: float, halfspan: float = 10.0, n: int = 9) -> np.ndarray:\n",
    "    if not np.isfinite(rc0):\n",
    "        return np.linspace(lo, hi, n)\n",
    "    a, b = max(lo, rc0 - halfspan), min(hi, rc0 + halfspan)\n",
    "    return np.unique(np.r_[np.linspace(a, b, n), rc0])\n",
    "\n",
    "def _knob_from_leg(leg: dict) -> str:\n",
    "    ch = leg['chamber']\n",
    "    if leg['feed'] == 'gas':\n",
    "        return f'RCOT_gas_chamber{ch}'\n",
    "    if ch in (1,2,3):\n",
    "        return f'RCOT_chamber{ch}'\n",
    "    return f'RCOT_naphtha_chamber{ch}'\n",
    "\n",
    "def build_knob_fidelity_gate(*, ts, row_current, gp, X_12h, merged_lims, pipeline, ml_cached,\n",
    "                             rc_bounds_map, thr_corr=0.92, min_cov=0.20,\n",
    "                             halfspan_ok=10.0, halfspan_fallback=2.0):\n",
    "    \"\"\"\n",
    "    Returns: (bounds_by_knob: dict, knob_fid_df: DataFrame)\n",
    "      PASS → trust ±halfspan_ok (clipped to geometry bounds)\n",
    "      FAIL → try SRTO finite-difference fallback; if OK → ±halfspan_fallback; else FREEZE (rc0,rc0)\n",
    "    \"\"\"\n",
    "    comp_row = gp._comp_row_for_ts(merged_lims, ts)\n",
    "    legs = pipeline._chamber_legs_from_state(row_current, feed_thr=1.0)  # active legs only\n",
    "\n",
    "    rows, bounds_by_knob = [], {}\n",
    "    KEY = ['Ethylene','Propylene','RPG']  # key revenue drivers\n",
    "\n",
    "    for leg in legs:\n",
    "        geom = leg['geometry']  # 'LF_NAPH' | 'GF_GAS' | 'GF_HYB_NAPH'\n",
    "        knob = _knob_from_leg(leg)\n",
    "        rc0  = float(row_current.get(knob, np.nan))\n",
    "        lo, hi = rc_bounds_map.get(geom, (800.0, 895.0))\n",
    "        rc_grid = _local_rc_grid(rc0, lo, hi, halfspan=halfspan_ok, n=9)\n",
    "\n",
    "        # Local anchored curve with tuned α (already set on gp)\n",
    "        curve, _ = gpmod.anchored_curve_at_ts(\n",
    "            gp=gp, X_12h=X_12h, merged_lims=merged_lims, pipeline=pipeline, ml=ml_cached,\n",
    "            ts=ts, rcot_setter=_rcot_setter_single_knob(knob), rc_grid=rc_grid,\n",
    "            use_gp_delta=True, alpha=0.0\n",
    "        )\n",
    "\n",
    "        flags = []\n",
    "        for p in KEY:\n",
    "            sc, cov = _robust_slope_metrics(curve, p)\n",
    "            rows.append({'timestamp': ts, 'geometry': geom, 'chamber': leg['chamber'],\n",
    "                         'knob': knob, 'product': p, 'slope_corr': sc, 'sign_cov': cov})\n",
    "            flags.append((sc >= thr_corr) and (cov >= min_cov))\n",
    "\n",
    "        if all(flags):\n",
    "            a = max(lo, rc0 - halfspan_ok); b = min(hi, rc0 + halfspan_ok)\n",
    "            bounds_by_knob[knob] = (a, b)\n",
    "            continue\n",
    "\n",
    "        # Fallback: SRTO central difference at rc0 ± 5°C\n",
    "        try:\n",
    "            dC = 5.0\n",
    "            def _spot(rc):\n",
    "                r = row_current.copy(); r[knob] = float(np.clip(rc, lo, hi))\n",
    "                spot = pipeline.predict_spot_plant(r, comp_row, feed_thr=0.1)\n",
    "                return spot['totals_tph'] if spot.get('status') == 'ok' else {}\n",
    "            y_lo = _spot(rc0 - dC); y_hi = _spot(rc0 + dC)\n",
    "            fd_ok = all(abs(y_hi.get(k,0.0) - y_lo.get(k,0.0)) > 1e-6 for k in KEY)\n",
    "        except Exception:\n",
    "            fd_ok = False\n",
    "\n",
    "        if fd_ok:\n",
    "            a = max(lo, rc0 - halfspan_fallback); b = min(hi, rc0 + halfspan_fallback)\n",
    "            bounds_by_knob[knob] = (a, b)\n",
    "        else:\n",
    "            bounds_by_knob[knob] = (rc0, rc0)\n",
    "\n",
    "    return bounds_by_knob, pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def auto_alpha_until_pass_for_ts(\n",
    "    gp: gpmod.GPResiduals,\n",
    "    ts: pd.Timestamp,\n",
    "    row0: pd.Series,\n",
    "    X_12h: pd.DataFrame,\n",
    "    merged_lims: pd.DataFrame,\n",
    "    pipeline,\n",
    "    ml_cached,\n",
    "    thr_corr: float=0.92,\n",
    "    min_cov: float=0.20,\n",
    "    rc_points: int=15\n",
    ") -> tuple[dict, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    geoms = []\n",
    "    n = sum(float(row0.get(f'Naphtha_chamber{i}',0.0)) for i in range(1,7))\n",
    "    g = sum(float(row0.get(f'Gas Feed_chamber{i}',0.0)) for i in (4,5,6))\n",
    "    if n > 1.0: geoms.append('LF_NAPH')\n",
    "    if g > 1.0: geoms.append('GF_GAS')\n",
    "    if n > 1.0 and g > 1.0: geoms.append('GF_HYB_NAPH')\n",
    "\n",
    "    setter_map = {'LF_NAPH':gpmod.rcot_setter_lf_naph, 'GF_GAS':gpmod.rcot_setter_gf_gas, 'GF_HYB_NAPH':gpmod.rcot_setter_hybrid}\n",
    "    rc_bounds  = _bounds_for_geoms()\n",
    "    setter_map = {g:setter_map[g] for g in geoms}; rc_bounds = {g:rc_bounds[g] for g in geoms}\n",
    "\n",
    "    overrides: dict[tuple[str,str], float] = {}\n",
    "    alpha_grid = np.r_[np.linspace(0.35, 0.0, 8), 0.0]\n",
    "\n",
    "    # First pass: find failing pairs\n",
    "    gp.set_alpha_overrides({})\n",
    "    fid_rows = []\n",
    "    for g in geoms:\n",
    "        lo, hi = rc_bounds[g]; rc_grid = _rc_grid_for(row0, g, lo, hi, points=rc_points)\n",
    "        curve, _ = gpmod.anchored_curve_at_ts(\n",
    "            gp=gp, X_12h=X_12h, merged_lims=merged_lims, pipeline=pipeline,\n",
    "            ml=ml_cached, ts=ts, rcot_setter=setter_map[g], rc_grid=rc_grid,\n",
    "            use_gp_delta=True, alpha=0.0\n",
    "        )\n",
    "        for p in gpmod.PRODUCTS:\n",
    "            sc, cov = _robust_slope_metrics(curve, p)\n",
    "            fid_rows.append({'timestamp': ts, 'geometry': _norm_geom(g), 'product': p, 'slope_corr': sc, 'sign_cov': cov})\n",
    "    fid0 = pd.DataFrame(fid_rows)\n",
    "    fails = fid0.loc[(fid0['slope_corr'] < thr_corr) | (fid0['sign_cov'] < min_cov), ['product','geometry']].drop_duplicates()\n",
    "\n",
    "    # Iterate α for failing pairs\n",
    "    for prod, geom in fails.itertuples(index=False):\n",
    "        tcol = gpmod.TARGET_MAP[prod]\n",
    "        best = None\n",
    "        for a in alpha_grid:\n",
    "            trial = {**overrides, (_norm_geom(geom), tcol): float(a)}\n",
    "            gp.set_alpha_overrides(trial)\n",
    "            lo, hi = rc_bounds[geom]; rc_grid = _rc_grid_for(row0, geom, lo, hi, points=rc_points)\n",
    "            curve, _ = gpmod.anchored_curve_at_ts(\n",
    "                gp=gp, X_12h=X_12h, merged_lims=merged_lims, pipeline=pipeline,\n",
    "                ml=ml_cached, ts=ts, rcot_setter=setter_map[geom], rc_grid=rc_grid,\n",
    "                use_gp_delta=True, alpha=0.0\n",
    "            )\n",
    "            sc, cov = _robust_slope_metrics(curve, prod)\n",
    "            if sc >= thr_corr and cov >= min_cov:\n",
    "                best = float(a); break\n",
    "        overrides[(_norm_geom(geom), tcol)] = (0.0 if best is None else best)\n",
    "\n",
    "    # Final detail/summary with overrides\n",
    "    gp.set_alpha_overrides(overrides)\n",
    "    fid_rows = []\n",
    "    for g in geoms:\n",
    "        lo, hi = rc_bounds[g]; rc_grid = _rc_grid_for(row0, g, lo, hi, points=rc_points)\n",
    "        curve, _ = gpmod.anchored_curve_at_ts(\n",
    "            gp=gp, X_12h=X_12h, merged_lims=merged_lims, pipeline=pipeline,\n",
    "            ml=ml_cached, ts=ts, rcot_setter=setter_map[g], rc_grid=rc_grid,\n",
    "            use_gp_delta=True, alpha=0.0\n",
    "        )\n",
    "        for p in gpmod.PRODUCTS:\n",
    "            sc, cov = _robust_slope_metrics(curve, p)\n",
    "            a_col, c_col = f'{p}_ANCHOR_tph', f'{p}_CORR_tph'\n",
    "            diff = (curve[c_col].astype(float) - curve[a_col].astype(float)).abs().to_numpy(float) if (a_col in curve and c_col in curve) else np.array([np.nan])\n",
    "            am_min = float(np.nanmin(diff))\n",
    "            fid_rows.append({'timestamp': ts, 'geometry': _norm_geom(g), 'product': p,\n",
    "                             'slope_corr': sc, 'sign_cov': cov, 'anchor_miss_min': am_min})\n",
    "    fid = pd.DataFrame(fid_rows)\n",
    "    fid['slope_ok'] = (fid['slope_corr'] >= thr_corr) & (fid['sign_cov'] >= min_cov)\n",
    "    summ = (fid.groupby(['product','geometry'])['slope_ok']\n",
    "                .agg(pct_ok=lambda s: float(100.0*s.mean()), n='count')\n",
    "                .reset_index()\n",
    "                .sort_values(['product','geometry']))\n",
    "    return overrides, fid, summ\n",
    "\n",
    "# =================== Counterfactual schedule & simulate ===================\n",
    "def build_rcot_schedule_from_recs(rec_df: pd.DataFrame) -> list[tuple[pd.Timestamp, dict]]:\n",
    "    if 'timestamp' in rec_df.columns:\n",
    "        df = rec_df.set_index('timestamp')\n",
    "    else:\n",
    "        df = rec_df.copy()\n",
    "    df = df.sort_index()\n",
    "    knobs_cols = [c for c in df.columns if c.startswith('rcot_opt_')]\n",
    "    sched = []\n",
    "    for ts, r in df.iterrows():\n",
    "        setpoints = {}\n",
    "        for c in knobs_cols:\n",
    "            v = r.get(c, np.nan)\n",
    "            if pd.notna(v):\n",
    "                knob = c.replace('rcot_opt_', '')\n",
    "                setpoints[knob] = float(v)\n",
    "        if setpoints:\n",
    "            sched.append((pd.Timestamp(ts), setpoints))\n",
    "    return sched\n",
    "\n",
    "def apply_schedule_to_X(X_12h: pd.DataFrame, schedule: list[tuple[pd.Timestamp, dict]],\n",
    "                        start: pd.Timestamp | None = None,\n",
    "                        end:   pd.Timestamp | None = None,\n",
    "                        hold: str = \"hold_until_next\") -> pd.DataFrame:\n",
    "    X_sim = X_12h.copy(); idx = X_sim.index\n",
    "    if start is None: start = idx.min()\n",
    "    if end   is None: end   = idx.max()\n",
    "    schedule = sorted([(pd.Timestamp(t), d) for (t,d) in schedule if start <= pd.Timestamp(t) <= end],\n",
    "                      key=lambda x: x[0])\n",
    "    if not schedule:\n",
    "        return X_sim\n",
    "    schedule2 = schedule + [(pd.Timestamp(end) + pd.Timedelta(seconds=1), {})]\n",
    "    for (t0, setpoints), (t1, _) in zip(schedule2[:-1], schedule2[1:]):\n",
    "        mask = (idx >= t0) & (idx < t1)\n",
    "        if not mask.any(): continue\n",
    "        for knob, val in setpoints.items():\n",
    "            if knob in X_sim.columns:\n",
    "                X_sim.loc[mask, knob] = float(val)\n",
    "    return X_sim\n",
    "\n",
    "def simulate_path_corrected(X_sim: pd.DataFrame,\n",
    "                            merged_lims: pd.DataFrame,\n",
    "                            pipeline,\n",
    "                            gp: gpmod.GPResiduals,\n",
    "                            gps_dict: dict,\n",
    "                            feature_cols_gp: list[str],\n",
    "                            price_provider: opt.PriceProvider,\n",
    "                            total_spyro_yield_for_now,\n",
    "                            fg_consts: opt.FuelGasConstants,\n",
    "                            alpha_overrides: dict | None = None,\n",
    "                            start: pd.Timestamp | None = None,\n",
    "                            end:   pd.Timestamp | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    idx = X_sim.index\n",
    "    if start is None: start = idx.min()\n",
    "    if end   is None: end   = idx.max()\n",
    "    stamps = idx[(idx >= start) & (idx <= end)]\n",
    "\n",
    "    margin_fn = opt.make_margin_fn_excel_delta(price_provider=price_provider,\n",
    "                                               total_spyro_yield_for_now=total_spyro_yield_for_now,\n",
    "                                               spyro_ctx=None, fg_constants=fg_consts)\n",
    "\n",
    "    Y_rows, M_rows = [], []\n",
    "    for ts in stamps:\n",
    "        row = X_sim.loc[ts].copy(); row.name = ts\n",
    "        y   = opt.corrected_yields_for_row(row, gps_dict, feature_cols_gp,\n",
    "                                           total_spyro_yield_for_now, spyro_ctx=None,\n",
    "                                           alpha_overrides=alpha_overrides)\n",
    "        Y_rows.append({'timestamp': ts, **y})\n",
    "        M_rows.append({'timestamp': ts, 'margin_per_h': float(margin_fn(ts, row, y))})\n",
    "\n",
    "    Y_sim = pd.DataFrame(Y_rows).set_index('timestamp').sort_index()\n",
    "    M_sim = pd.DataFrame(M_rows).set_index('timestamp').sort_index()\n",
    "    return Y_sim, M_sim\n",
    "\n",
    "def _cache_path(prefix: str, cache_tag: str, mode: str, train_mode: str, ext: str) -> Path:\n",
    "    tag = cache_tag or \"\"\n",
    "    return OUT_DIR / f\"{prefix}{tag}_{mode}_{train_mode}.{ext}\"\n",
    "\n",
    "def effective_target_cols(Y_12h: pd.DataFrame, explicit: Optional[Sequence[str]] = None) -> List[str]:\n",
    "    from_cols = [c for c in Y_12h.columns if c.endswith(\"_prod_t+1\")]\n",
    "    if explicit is None:\n",
    "        return sorted(set(from_cols))\n",
    "    return sorted(set(explicit) | set(from_cols))\n",
    "\n",
    "def seed_sim_state(Y_12h: pd.DataFrame, tcols: Sequence[str], seed_until: pd.Timestamp) -> pd.DataFrame:\n",
    "    ys = pd.DataFrame(index=Y_12h.index, columns=list(tcols), dtype=float)\n",
    "    common = [c for c in tcols if c in Y_12h.columns]\n",
    "    ys.loc[:, common] = Y_12h[common]\n",
    "    ys.loc[ys.index > seed_until, :] = np.nan\n",
    "    return ys\n",
    "\n",
    "def ensure_ml_preds_for(\n",
    "    stamps: Sequence[pd.Timestamp],\n",
    "    Xsrc: pd.DataFrame,\n",
    "    Ysrc: pd.DataFrame,\n",
    "    lookback: pd.Timedelta,\n",
    "    target_cols: Sequence[str],\n",
    "    *,\n",
    "    mode: str,                 # 'historical' | 'closed_loop'\n",
    "    train_mode: str,           # 'historical' | 'simulated'\n",
    "    cache_tag: str = \"\",\n",
    "    Y_sim_state: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Predict ML targets for `stamps`, keeping a cache per (mode, train_mode, cache_tag).\n",
    "    - train_mode=historical  → fit on Ysrc (ground truth)\n",
    "    - train_mode=simulated   → fit on Y_sim_state (simulated truth)\n",
    "    - Lags always read from (Y_sim_state ⊕ Ysrc) during closed-loop to avoid KeyErrors.\n",
    "    \"\"\"\n",
    "    cache_csv = _cache_path(\"ml_cache\", cache_tag, mode, train_mode, \"csv\")\n",
    "    if cache_csv.exists():\n",
    "        pred_cache = pd.read_csv(cache_csv, parse_dates=[\"timestamp\"]).set_index(\"timestamp\")\n",
    "    else:\n",
    "        pred_cache = pd.DataFrame(index=pd.DatetimeIndex([], name=\"timestamp\"))\n",
    "\n",
    "    missing = sorted(ts for ts in stamps if ts not in pred_cache.index)\n",
    "    if not missing:\n",
    "        return pred_cache\n",
    "\n",
    "    rows = []\n",
    "    tcols = list(target_cols)\n",
    "\n",
    "    for ts in missing:\n",
    "        ts_start = ts - lookback\n",
    "        tr = Xsrc.index[(Xsrc.index >= ts_start) & (Xsrc.index < ts)]\n",
    "        if len(tr) < MIN_TR_ROWS:\n",
    "            # not enough history to fit\n",
    "            rows.append({\"timestamp\": ts, **{c: np.nan for c in tcols}})\n",
    "            continue\n",
    "\n",
    "        Y_train = (Y_sim_state if (train_mode == \"simulated\" and Y_sim_state is not None) else Ysrc)\n",
    "\n",
    "        ml = MLPredictor(\n",
    "            target_cols=tcols,\n",
    "            cfg=MLPredictorConfig(\n",
    "                ds_prefixes=[\"DS_chamber\"],\n",
    "                add_virtual_rcots=True,\n",
    "                build_lag1_from_targets=True,\n",
    "                lgbm_params=dict(verbosity=-1, n_jobs=2),\n",
    "            ),\n",
    "        ).fit(Xsrc.loc[tr], Y_train.loc[tr])\n",
    "\n",
    "        # choose lag source (combine so missing columns are tolerated)\n",
    "        if mode == \"closed_loop\" and Y_sim_state is not None:\n",
    "            Y_lag_src = Y_sim_state.combine_first(Ysrc)\n",
    "        else:\n",
    "            Y_lag_src = Ysrc\n",
    "\n",
    "        pm = ml.predict_row(Xsrc.loc[ts], Y_for_lags=Y_lag_src)\n",
    "        rows.append({\"timestamp\": ts, **{c: float(pm.get(c, np.nan)) for c in tcols}})\n",
    "\n",
    "    if rows:\n",
    "        add_df = pd.DataFrame(rows).set_index(\"timestamp\")\n",
    "        pred_cache = pd.concat([pred_cache, add_df], axis=0).sort_index()\n",
    "        pred_cache.to_csv(cache_csv, index=True)\n",
    "\n",
    "    return pred_cache\n",
    "\n",
    "def _legs_to_df(legs: list[dict]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    keep = ['Ethylene','Propylene','MixedC4','RPG','PFO','Ethane','Propane','Tail_Gas','Hydrogen']\n",
    "    for lg in legs:\n",
    "        rec = {\n",
    "            'chamber': lg.get('chamber'),\n",
    "            'feed': lg.get('feed'),\n",
    "            'geometry': lg.get('geometry'),\n",
    "            'rcot_used': float(lg.get('rcot_used', np.nan)),\n",
    "            'feed_tph': float(lg.get('feed_tph', 0.0)),\n",
    "            'IRET': float(lg.get('IRET', np.nan))\n",
    "        }\n",
    "        for k in keep:\n",
    "            rec[f'{k}_tph'] = float(lg.get('tph', {}).get(k, 0.0))\n",
    "        rows.append(rec)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _ch_from_knob(knob: str) -> int | None:\n",
    "    if knob.startswith('RCOT_chamber'):            return int(knob.replace('RCOT_chamber',''))\n",
    "    if knob.startswith('RCOT_naphtha_chamber'):    return int(knob.replace('RCOT_naphtha_chamber',''))\n",
    "    if knob.startswith('RCOT_gas_chamber'):        return int(knob.replace('RCOT_gas_chamber',''))\n",
    "    return None\n",
    "\n",
    "# =================== RUNNER ===================\n",
    "def run_production(X_12h: pd.DataFrame, Y_12h: pd.DataFrame,\n",
    "                   merged_lims: pd.DataFrame, pipeline,\n",
    "                   prices_df: pd.DataFrame,\n",
    "                   total_spyro_yield_for_now,\n",
    "                   start: pd.Timestamp,\n",
    "                   end:   pd.Timestamp | None = None,\n",
    "                   mode: str = 'historical',\n",
    "                   closed_loop_opts: dict | None = None):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - 'historical': baseline behavior\n",
    "      - 'closed_loop': step & hold recommended RCOTs on a stateful copy of X;\n",
    "                       uses simulated lags for ML/inference; separate caches via cache_tag.\n",
    "    \"\"\"\n",
    "    assert mode in ('historical','closed_loop')\n",
    "    closed_loop_opts = closed_loop_opts or {}\n",
    "    apply_timing  = closed_loop_opts.get('apply_timing', 'next_day')\n",
    "    hold_policy   = closed_loop_opts.get('hold_policy',  'hold_until_next')\n",
    "    ml_train_mode = closed_loop_opts.get('ml_train_mode','historical')\n",
    "    gp_train_mode = closed_loop_opts.get('gp_train_mode','historical')\n",
    "    cache_tag     = closed_loop_opts.get('cache_tag',    '' if mode=='historical' else '_sim')\n",
    "\n",
    "    # Out dirs & caches\n",
    "    OUT_DIR = OUT_DIR_BASE / (('closed_loop' + cache_tag) if mode=='closed_loop' else 'historical')\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CURVES_DIR = OUT_DIR / \"curves\"; CURVES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    FID_DIR    = OUT_DIR / \"fidelity\"; FID_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    MOV_DIR    = OUT_DIR / \"rcot_moves\"; MOV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    AK_DIR     = OUT_DIR / \"active_knobs\"; AK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    AUD_DIR    = OUT_DIR / \"audits\"; AUD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    SNAP_DIR   = OUT_DIR / \"multi_snapshots\"; SNAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    CF_DIR     = OUT_DIR / \"counterfactual\"; CF_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    KNOB_FID_DIR = FID_DIR / \"knob\"; KNOB_FID_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ATTR_DIR   = OUT_DIR / \"attribution\"; ATTR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pp = opt.PriceProvider(prices_df)\n",
    "    gpmod.set_rcot_groups_from_columns(X_12h.columns)\n",
    "\n",
    "    # fuel-gas constants\n",
    "    fg_consts = opt.FuelGasConstants(\n",
    "        rc_ref_naph=840.0, rc_ref_gas=880.0,\n",
    "        cp_wavg_kcal_per_ton_K=411.488209,\n",
    "        dH_eth_kcal_per_ton=1_080_970.0,\n",
    "        dH_prop_kcal_per_ton=673_409.0,\n",
    "        dH_fg_kcal_per_ton=926_147.0,\n",
    "        fuel_gas_heat_content_kcal_per_ton=15_294_088.0\n",
    "    )\n",
    "\n",
    "    idx_all = X_12h.index.sort_values()\n",
    "    if end is None: end = idx_all.max().normalize()\n",
    "\n",
    "    # Closed-loop state\n",
    "    X_state = X_12h.copy()\n",
    "    tcols_for_sim = [c for c in TARGET_COLS if c in Y_12h.columns]\n",
    "    Y_sim_state = (Y_12h.reindex(columns=tcols_for_sim).copy() if mode=='closed_loop' else None)\n",
    "\n",
    "    # ---------- GP training cache helper ----------\n",
    "    def ensure_gp_train_for_window(train_start: pd.Timestamp, train_end: pd.Timestamp,\n",
    "                                   Xs: pd.DataFrame, Ys: pd.DataFrame,\n",
    "                                   merged_lims: pd.DataFrame, pipeline,\n",
    "                                   ml_cached: MLCacheAdapter) -> pd.DataFrame:\n",
    "        if (OUT_DIR / f\"gp_train_cache{cache_tag}.pkl\").exists():\n",
    "            gp_cache = pd.read_pickle(OUT_DIR / f\"gp_train_cache{cache_tag}.pkl\")\n",
    "            gp_cache.index = pd.to_datetime(gp_cache.index)\n",
    "            cached_idx = gp_cache.index\n",
    "        else:\n",
    "            gp_cache = pd.DataFrame(); cached_idx = pd.DatetimeIndex([])\n",
    "\n",
    "        idx_in_win = Xs.index[(Xs.index >= train_start) & (Xs.index <= train_end)]\n",
    "        need = [ts for ts in idx_in_win if ts not in cached_idx]\n",
    "        if need:\n",
    "            s = min(need); e = max(need)\n",
    "            Xsrc = (X_state if (mode=='closed_loop' and gp_train_mode=='simulated') else Xs)\n",
    "            Ysrc = (Y_sim_state if (mode=='closed_loop' and Y_sim_state is not None and gp_train_mode=='simulated') else Ys)\n",
    "            df_new = gpmod.GPResiduals.build_training_table(\n",
    "                Xsrc, Ysrc, merged_lims, pipeline,\n",
    "                start=s, end=e, feed_thr=0.1,\n",
    "                feature_cfg=gpmod.GPFeatureConfig(\n",
    "                    ds_prefixes=['DS_chamber'], rcot_prefixes=['RCOT_'],\n",
    "                    feed_prefixes=['Naphtha_chamber','Gas Feed_chamber'],\n",
    "                    include_ratio_naphtha=True, include_geometry_flags=True\n",
    "                ),\n",
    "                residual_kind='ml', ml=ml_cached\n",
    "            )\n",
    "            if not df_new.empty:\n",
    "                df_new.index = pd.to_datetime(df_new.index)\n",
    "                _safe_merge_pickle(OUT_DIR / f\"gp_train_cache{cache_tag}.pkl\", df_new, min_index=train_start)\n",
    "        if (OUT_DIR / f\"gp_train_cache{cache_tag}.pkl\").exists():\n",
    "            win = pd.read_pickle(OUT_DIR / f\"gp_train_cache{cache_tag}.pkl\")\n",
    "            win.index = pd.to_datetime(win.index)\n",
    "            win = win.loc[(win.index >= train_start) & (win.index <= train_end)].sort_index()\n",
    "        else:\n",
    "            win = pd.DataFrame()\n",
    "        return win\n",
    "\n",
    "    # ===== daily loop =====\n",
    "    for day in pd.date_range(pd.Timestamp(start).normalize(), pd.Timestamp(end).normalize(), freq='D'):\n",
    "        stamps = _ts_per_day(idx_all, day)\n",
    "        if not stamps:\n",
    "            continue\n",
    "        te = stamps[-1] - pd.Timedelta(hours=12)\n",
    "        ts_train_start = te - LOOKBACK_6M\n",
    "\n",
    "        # ---- ML cache for the training window (respect provenance) ----\n",
    "        tcols_ml = [c for c in TARGET_COLS if c in Y_12h.columns]\n",
    "        Xsrc_train = X_state if (mode=='closed_loop' and ml_train_mode=='simulated') else X_12h\n",
    "        train_stamps = list(X_12h.index[(X_12h.index >= ts_train_start) & (X_12h.index <= te)])\n",
    "        pred_cache_train = ensure_ml_preds_for(\n",
    "            train_stamps, Xsrc_train, Y_12h, lookback=LOOKBACK_6M, target_cols=tcols_ml,\n",
    "            mode=mode, train_mode=ml_train_mode, cache_tag=cache_tag,\n",
    "            Y_sim_state=(Y_sim_state if mode == 'closed_loop' else None),\n",
    "        )\n",
    "        ml_cached_train = MLCacheAdapter(pred_cache_train)\n",
    "\n",
    "        # ---- GP train cache ----\n",
    "        df_train_win = ensure_gp_train_for_window(ts_train_start, te, X_12h, Y_12h, merged_lims, pipeline, ml_cached_train)\n",
    "        if df_train_win.empty:\n",
    "            continue\n",
    "\n",
    "        gp = gpmod.GPResiduals(\n",
    "            feature_cfg=gpmod.GPFeatureConfig(\n",
    "                ds_prefixes=['DS_chamber'], rcot_prefixes=['RCOT_'],\n",
    "                feed_prefixes=['Naphtha_chamber','Gas Feed_chamber'],\n",
    "                include_ratio_naphtha=True, include_geometry_flags=True\n",
    "            ),\n",
    "            n_restarts=2, normalize_y=True\n",
    "        ).fit_parallel(df_train_win, n_jobs=GP_JOBS)\n",
    "\n",
    "        gps_dict = {f'{p}_prod_t+1': gp.models_[p] for p in gpmod.PRODUCTS if p in gp.models_}\n",
    "        feature_cols_gp = gp.feature_names_\n",
    "\n",
    "        rec_rows = []\n",
    "        for ts in stamps:\n",
    "            # ---- current row from state ----\n",
    "            row_current = (X_state if mode=='closed_loop' else X_12h).loc[ts].copy(); row_current.name = ts\n",
    "\n",
    "            # ensure Spyro sees a timestamp\n",
    "            def _spyro_ts(row_like, short_key, ctx=None, _ts=ts):\n",
    "                r = row_like\n",
    "                if getattr(r, 'name', None) is None:\n",
    "                    r = r.copy(); r.name = _ts\n",
    "                return total_spyro_yield_for_now(r, short_key, ctx)\n",
    "\n",
    "            geom = _geometry_label_for_row(row_current)\n",
    "            naph_b, gas_b = RC_BOUNDS.get('LF_NAPH'), RC_BOUNDS.get('GF_GAS')\n",
    "\n",
    "            margin_fn = opt.make_margin_fn_excel_delta(\n",
    "                price_provider=pp,\n",
    "                total_spyro_yield_for_now=_spyro_ts,\n",
    "                spyro_ctx=None,\n",
    "                fg_constants=fg_consts\n",
    "            )\n",
    "\n",
    "            # ---- spot ML baseline using UPDATED state ----\n",
    "            pred_cache_spot = ensure_ml_preds_for(\n",
    "                [ts], (X_state if mode=='closed_loop' else X_12h), Y_12h,\n",
    "                lookback=LOOKBACK_6M, target_cols=tcols_ml,\n",
    "                mode=mode, train_mode=ml_train_mode, cache_tag=cache_tag,\n",
    "                Y_sim_state=(Y_sim_state if mode == 'closed_loop' else None),\n",
    "            )\n",
    "            ml_cached = MLCacheAdapter(pred_cache_spot)\n",
    "            y0 = ml_cached.predict_row(row_current)\n",
    "            m0 = margin_fn(ts, row_current, y0)\n",
    "            m_real = realized_margin_from_Y(ts, row_current, Y_12h, margin_fn)\n",
    "\n",
    "            # ---- fidelity + knob gate ----\n",
    "            overrides, fid_detail, fid_summary = auto_alpha_until_pass_for_ts(\n",
    "                gp=gp, ts=ts, row0=row_current, X_12h=X_12h,\n",
    "                merged_lims=merged_lims, pipeline=pipeline, ml_cached=ml_cached,\n",
    "                thr_corr=0.92, min_cov=0.20, rc_points=15\n",
    "            )\n",
    "            FID_DIR.mkdir(exist_ok=True, parents=True)\n",
    "            fid_detail.to_csv((OUT_DIR / \"fidelity\" / f\"fidelity_detail_{ts:%Y%m%d_%H%M}.csv\"), index=False)\n",
    "            fid_summary.to_csv((OUT_DIR / \"fidelity\" / f\"fidelity_summary_{ts:%Y%m%d_%H%M}.csv\"), index=False)\n",
    "\n",
    "            bounds_by_knob, knob_fid = build_knob_fidelity_gate(\n",
    "                ts=ts, row_current=row_current, gp=gp,\n",
    "                X_12h=X_12h, merged_lims=merged_lims, pipeline=pipeline, ml_cached=ml_cached,\n",
    "                rc_bounds_map=RC_BOUNDS, thr_corr=0.92, min_cov=0.20,\n",
    "                halfspan_ok=10.0, halfspan_fallback=2.0\n",
    "            )\n",
    "            (OUT_DIR / \"fidelity\" / \"knob\").mkdir(exist_ok=True, parents=True)\n",
    "            knob_fid.to_csv(OUT_DIR / \"fidelity\" / \"knob\" / f\"fidelity_knob_{ts:%Y%m%d_%H%M}.csv\", index=False)\n",
    "\n",
    "            # ---- Optimize (anchored objective) ----\n",
    "            res = opt.optimize_rcot_for_ts_multi(\n",
    "                ts=ts, row0=row_current,\n",
    "                gps=gps_dict, feature_cols_gp=feature_cols_gp,\n",
    "                total_spyro_yield_for_now=total_spyro_yield_for_now, spyro_ctx=None,\n",
    "                price_provider=pp,\n",
    "                objective='per_hour',\n",
    "                naph_bounds=naph_b, gas_bounds=gas_b,\n",
    "                trust_delta_C=5.0,                     # your ±5 °C trust window\n",
    "                use_recycle_fixed_point=False, recycle_fn=None,\n",
    "                margin_mode='excel_delta', fg_constants=fg_consts,\n",
    "                alpha_overrides=None,\n",
    "                enable_de=True, enable_slsqp=True,\n",
    "                bounds_by_knob=bounds_by_knob,\n",
    "                anchored_from_ml=True,                 # ← anchored mode\n",
    "                gp=gp, pipeline=pipeline, merged_lims=merged_lims, ml_cached=ml_cached,\n",
    "                alpha_default_for_anchor=0.0\n",
    "            )\n",
    "\n",
    "            row_opt = res['row_opt']; y_opt = res['yields_opt']\n",
    "            m_base  = res['margin_current_per_h']; m_opt = res['margin_opt_per_h']; d_m = res['improvement_per_h']\n",
    "\n",
    "            print(\"\\n=== MULTI-KNOB RESULT ===\")\n",
    "            print(\"Status:\", res.get('status'))\n",
    "            print(\"ΔMargin $/h:\", f\"{d_m:,.2f}\")\n",
    "            print(\"RCOT* (per chamber):\")\n",
    "            for k,v in res['rcot_opt'].items():\n",
    "                cur = float(row_current.get(k, np.nan))\n",
    "                print(f\"  {k:24s}  {cur:7.2f} → {float(v):7.2f}  (Δ {float(v)-cur:+.2f})\")\n",
    "\n",
    "            # ---- Flat record ----\n",
    "            flat = {\n",
    "                'timestamp': ts, 'geometry': geom, 'status': res['status'],\n",
    "                'margin_baseline_per_h': float(m0),\n",
    "                'margin_opt_per_h':      float(m_opt),\n",
    "                'improvement_per_h':     float(d_m),\n",
    "                'margin_realized_per_h': float(m_real),\n",
    "                'margin_current_per_h':  float(m_base),\n",
    "            }\n",
    "            rcot_cols = [f'RCOT_chamber{i}' for i in (1,2,3)] \\\n",
    "                      + [f'RCOT_naphtha_chamber{i}' for i in (4,5,6)] \\\n",
    "                      + [f'RCOT_gas_chamber{i}'     for i in (4,5,6)]\n",
    "            for c in rcot_cols:\n",
    "                flat[f'rcot_current_{c}'] = float(row_current.get(c, np.nan))\n",
    "            for k, v in res['rcot_opt'].items():\n",
    "                flat[f'rcot_opt_{k}'] = float(v)\n",
    "            for p in ['Ethylene','Propylene','MixedC4','RPG','Hydrogen','Tail_Gas']:\n",
    "                flat[f'{p}_current_tph'] = float(y0.get(f'{p}_prod_t+1', np.nan))\n",
    "                flat[f'{p}_opt_tph']     = float(y_opt.get(f'{p}_prod_t+1', np.nan))\n",
    "                flat[f'{p}_delta_tph']   = flat[f'{p}_opt_tph'] - flat[f'{p}_current_tph']\n",
    "\n",
    "            (OUT_DIR / \"multi_snapshots\").mkdir(exist_ok=True, parents=True)\n",
    "            snap = dict(\n",
    "                timestamp=str(ts), status=res.get('status'),\n",
    "                margins=dict(current=m_base, optimal=m_opt, delta=d_m),\n",
    "                rcot_opt=res.get('rcot_opt', {}),\n",
    "                yields_current={k: float(y0.get(k, np.nan)) for k in y0.keys()},\n",
    "                yields_opt={k: float(y_opt.get(k, np.nan)) for k in y_opt.keys()},\n",
    "            )\n",
    "            (OUT_DIR / \"multi_snapshots\" / f\"multi_{ts:%Y%m%d_%H%M}.json\").write_text(json.dumps(snap, indent=2))\n",
    "\n",
    "            # ---- Attribute (anchored deltas) ----\n",
    "            def _flow_for_knob(knob: str, r: pd.Series) -> float:\n",
    "                if knob.startswith('RCOT_chamber'):\n",
    "                    ch = int(knob.replace('RCOT_chamber',''))\n",
    "                    return float(r.get(f'Naphtha_chamber{ch}', 0.0))\n",
    "                if knob.startswith('RCOT_naphtha_chamber'):\n",
    "                    ch = int(knob.replace('RCOT_naphtha_chamber',''))\n",
    "                    return float(r.get(f'Naphtha_chamber{ch}', 0.0))\n",
    "                if knob.startswith('RCOT_gas_chamber'):\n",
    "                    ch = int(knob.replace('RCOT_gas_chamber',''))\n",
    "                    return float(r.get(f'Gas Feed_chamber{ch}', 0.0))\n",
    "                return 0.0\n",
    "\n",
    "            rcot_knobs = rcot_cols\n",
    "            moves = []\n",
    "            for knob in rcot_knobs:\n",
    "                rc_curr = float(row_current.get(knob, np.nan))\n",
    "                rc_opt  = float(res['rcot_opt'].get(knob, rc_curr))\n",
    "                flow    = _flow_for_knob(knob, row_current)\n",
    "                active_flag = bool((flow > 1.0) and np.isfinite(rc_curr) and (rc_curr >= 800.0))\n",
    "                moves.append(dict(timestamp=ts, knob=knob, flow_tph=flow,\n",
    "                                  rcot_current_C=rc_curr, rcot_opt_C=rc_opt,\n",
    "                                  delta_C=(rc_opt - rc_curr), active=active_flag))\n",
    "            df_moves = pd.DataFrame(moves)\n",
    "            (OUT_DIR / \"rcot_moves\").mkdir(exist_ok=True, parents=True)\n",
    "            if not df_moves.empty:\n",
    "                df_moves.to_csv(OUT_DIR / \"rcot_moves\" / f\"rcot_moves_{ts:%Y%m%d_%H%M}.csv\", index=False)\n",
    "                MOVES_CSV = OUT_DIR / \"rcot_moves_all.csv\"\n",
    "                df_moves.to_csv(MOVES_CSV, mode='a', header=not Path(MOVES_CSV).exists(), index=False)\n",
    "\n",
    "            # ---- Prices ----\n",
    "            def _prices_at(ts: pd.Timestamp, pp: opt.PriceProvider):\n",
    "                pr_c = {p: float(pp.get(ts, {'Ethylene':'Ethylene','Propylene':'Propylene','MixedC4':'Mixed C4','RPG':'RPG','Hydrogen':'Hydrogen','Tail_Gas':'Tail Gas'}[p], 0.0))\n",
    "                        for p in ['Ethylene','Propylene','MixedC4','RPG','Hydrogen','Tail_Gas']}\n",
    "                pr_f = {\n",
    "                    'PN':        float(pp.get(ts, 'PN', 0.0)),\n",
    "                    'Gas Feed':  float(pp.get(ts, 'Gas Feed', 0.0)),\n",
    "                    'LPG':       float(pp.get(ts, 'LPG', float(pp.get(ts,'Gas Feed',0.0)))),\n",
    "                    'MX Offgas': float(pp.get(ts, 'MX Offgas', 0.0)),\n",
    "                    'Fuel Gas':  float(pp.get(ts, 'Fuel Gas', 0.0)),\n",
    "                    'Tail Gas':  float(pp.get(ts, 'Tail Gas', 0.0)),\n",
    "                }\n",
    "                return pr_c, pr_f\n",
    "\n",
    "            pr_c, pr_f = _prices_at(ts, pp)\n",
    "\n",
    "            def _revenue(yields_abs, pr_c):\n",
    "                return sum(float(yields_abs.get(f'{p}_prod_t+1',0.0))*pr_c[p] for p in ['Ethylene','Propylene','MixedC4','RPG','Hydrogen','Tail_Gas'])\n",
    "\n",
    "            def _feeds(r, pr_f):\n",
    "                naph = sum(float(r.get(f'Naphtha_chamber{i}',0.0)) for i in range(1,7))\n",
    "                gasC = sum(float(r.get(f'Gas Feed_chamber{i}',0.0)) for i in (4,5,6))\n",
    "                fresh_lpg = float(r.get('FreshFeed_C3 LPG',0.0))\n",
    "                fresh_off = float(r.get('FreshFeed_MX Offgas',0.0))\n",
    "                if fresh_lpg>0 or fresh_off>0:\n",
    "                    return naph*pr_f['PN'] + fresh_lpg*pr_f['LPG'] + fresh_off*pr_f['MX Offgas']\n",
    "                return naph*pr_f['PN'] + gasC*pr_f['Gas Feed']\n",
    "\n",
    "            def _recycle(yields_abs, pr_f):\n",
    "                eth = float(yields_abs.get('Ethane_prod_t+1',0.0)); pro = float(yields_abs.get('Propane_prod_t+1',0.0))\n",
    "                return (eth + pro) * pr_f['LPG']\n",
    "\n",
    "            # ΔFG energy fixed baseline\n",
    "            def _per_side_rc_eff(r: pd.Series):\n",
    "                def _mean(keys, fkeys, rc_min=800.0, flow_thr=1.0):\n",
    "                    pairs = []\n",
    "                    for k, fk in zip(keys, fkeys):\n",
    "                        rc = float(r.get(k, np.nan)); f = float(r.get(fk, 0.0))\n",
    "                        if np.isfinite(rc) and rc >= rc_min and f > flow_thr:\n",
    "                            pairs.append((rc, f))\n",
    "                    if not pairs: return np.nan\n",
    "                    w = sum(f for _, f in pairs)\n",
    "                    return sum(rc*f for rc,f in pairs)/w if w>0 else np.nan\n",
    "                rc13   = _mean([f'RCOT_chamber{i}'         for i in (1,2,3)],\n",
    "                               [f'Naphtha_chamber{i}'      for i in (1,2,3)])\n",
    "                rcn456 = _mean([f'RCOT_naphtha_chamber{i}' for i in (4,5,6)],\n",
    "                               [f'Naphtha_chamber{i}'      for i in (4,5,6)])\n",
    "                rcg456 = _mean([f'RCOT_gas_chamber{i}'     for i in (4,5,6)],\n",
    "                               [f'Gas Feed_chamber{i}'     for i in (4,5,6)])\n",
    "                rc_n_eff = np.nanmean([rc13, rcn456]) if (np.isfinite(rc13) or np.isfinite(rcn456)) else np.nan\n",
    "                rc_g_eff = rcg456\n",
    "                naph = sum(float(r.get(f'Naphtha_chamber{i}', 0.0))  for i in range(1,7))\n",
    "                gas  = sum(float(r.get(f'Gas Feed_chamber{i}', 0.0)) for i in (4,5,6))\n",
    "                D    = max(float(naph + gas), 1e-9)\n",
    "                wN, wG = (naph / D), (gas / D)\n",
    "                return rc_n_eff, rc_g_eff, wN, wG, D\n",
    "\n",
    "            def _fg_abs(r: pd.Series):\n",
    "                for sk in ('Fuel_Gas','Fuel Gas','FG','FuelGas','Tail Gas','Tail_Gas'):\n",
    "                    try: return float(_spyro_ts(r, sk, ctx=None))\n",
    "                    except Exception: pass\n",
    "                return 0.0\n",
    "\n",
    "            def _override_rcot(r: pd.Series, rc_n=None, rc_g=None) -> pd.Series:\n",
    "                rr = r.copy()\n",
    "                if rc_n is not None:\n",
    "                    for ch in (1,2,3): rr[f'RCOT_chamber{ch}'] = float(rc_n)\n",
    "                    for ch in (4,5,6): rr[f'RCOT_naphtha_chamber{ch}'] = float(rc_n)\n",
    "                if rc_g is not None:\n",
    "                    for ch in (4,5,6): rr[f'RCOT_gas_chamber{ch}'] = float(rc_g)\n",
    "                return rr\n",
    "\n",
    "            rc_n0, rc_g0, _, _, _ = _per_side_rc_eff(row_current)\n",
    "\n",
    "            def _dfg_energy(row_like, yields_abs, rc_n0, rc_g0):\n",
    "                rc_n, rc_g, wN, wG, D = _per_side_rc_eff(row_like)\n",
    "                if not np.isfinite(rc_n): rc_n = float(fg_consts.rc_ref_naph)\n",
    "                if not np.isfinite(rc_g): rc_g = float(fg_consts.rc_ref_gas)\n",
    "                E_abs = float(yields_abs.get('Ethylene_prod_t+1',0.0))\n",
    "                P_abs = float(yields_abs.get('Propylene_prod_t+1',0.0))\n",
    "                FG_ab = _fg_abs(row_like)\n",
    "                rE, rP, rFG = E_abs/D, P_abs/D, FG_ab/D\n",
    "                r_base = _override_rcot(row_like, rc_n=rc_n0, rc_g=rc_g0)\n",
    "                base_E = float(_spyro_ts(r_base, 'Ethylene', None))/D\n",
    "                base_P = float(_spyro_ts(r_base, 'Propylene', None))/D\n",
    "                base_FG = _fg_abs(r_base)/D\n",
    "                Cpw = float(fg_consts.cp_wavg_kcal_per_ton_K); HHV = float(fg_consts.fuel_gas_heat_content_kcal_per_ton)\n",
    "                rc_term = Cpw * (wN*(rc_n-rc_n0) + wG*(rc_g-rc_g0))\n",
    "                S = rc_term + (rE-base_E)*float(fg_consts.dH_eth_kcal_per_ton) \\\n",
    "                            + (rP-base_P)*float(fg_consts.dH_prop_kcal_per_ton) \\\n",
    "                            + (rFG-base_FG)*float(fg_consts.dH_fg_kcal_per_ton)\n",
    "                return float((S/HHV)*D)\n",
    "\n",
    "            # ---- Summary/Audit ----\n",
    "            items = pd.DataFrame([\n",
    "                dict(product=p.replace('MixedC4','Mixed C4').replace('Tail_Gas','Tail Gas'),\n",
    "                     qty_before=float(y0.get(f'{p}_prod_t+1',0.0)),\n",
    "                     qty_after= float(y_opt.get(f'{p}_prod_t+1',0.0)),\n",
    "                     price=pr_c[p],\n",
    "                     rev_before=float(y0.get(f'{p}_prod_t+1',0.0))*pr_c[p],\n",
    "                     rev_after=float(y_opt.get(f'{p}_prod_t+1',0.0))*pr_c[p],\n",
    "                     rev_delta=(float(y_opt.get(f'{p}_prod_t+1',0.0))-float(y0.get(f'{p}_prod_t+1',0.0)))*pr_c[p]\n",
    "                     )\n",
    "                for p in ['Ethylene','Propylene','MixedC4','RPG','Hydrogen','Tail_Gas']\n",
    "            ])\n",
    "\n",
    "            rev_b = _revenue(y0, pr_c);     rev_a = _revenue(y_opt, pr_c)\n",
    "            feed_b= _feeds(row_current, pr_f);     feed_a= _feeds(row_opt, pr_f)\n",
    "            rec_b = _recycle(y0, pr_f);     rec_a=  _recycle(y_opt, pr_f)\n",
    "            dfg_b = _dfg_energy(row_current, y0, rc_n0, rc_g0);   dfg_a = _dfg_energy(row_opt, y_opt, rc_n0, rc_g0)\n",
    "            fg_price = pr_f.get('Fuel Gas', pr_f.get('Tail Gas', 0.0))\n",
    "            fgc_b = dfg_b * fg_price;       fgc_a = dfg_a * fg_price\n",
    "            m_b   = rev_b - feed_b + rec_b - fgc_b\n",
    "            m_a   = rev_a - feed_a + rec_a - fgc_a\n",
    "\n",
    "            print(\"\\n=== PRICE SNAPSHOT @\", ts, \"===\")\n",
    "            print({\n",
    "                'Ethylene':  pr_c['Ethylene'],\n",
    "                'Propylene': pr_c['Propylene'],\n",
    "                'Mixed C4':  pr_c['MixedC4'],\n",
    "                'RPG':       pr_c['RPG'],\n",
    "                'Hydrogen':  pr_c['Hydrogen'],\n",
    "                'Tail Gas':  pr_f['Tail Gas'],\n",
    "                'Fuel Gas':  pr_f['Fuel Gas'],\n",
    "                'PN':        pr_f['PN'],\n",
    "                'Gas Feed':  pr_f['Gas Feed'],\n",
    "                'LPG':       pr_f['LPG'],\n",
    "                'MX Offgas': pr_f['MX Offgas'],\n",
    "            })\n",
    "\n",
    "            print(\"\\n=== BEFORE vs AFTER (per product) ===\")\n",
    "            print(items[['product','qty_before','qty_after','price','rev_before','rev_after','rev_delta']]\n",
    "                  .sort_values('product').to_string(index=False, float_format=lambda x: f\"{x:,.3f}\"))\n",
    "\n",
    "            print(\"\\n=== SUMMARY ($/h) ===\")\n",
    "            print(f\"Revenue      : {rev_b:,.2f}  →  {rev_a:,.2f}   (Δ {rev_a-rev_b:+,.2f})\")\n",
    "            print(f\"Feed cost    : {feed_b:,.2f} →  {feed_a:,.2f}  (Δ {feed_a-feed_b:+,.2f})\")\n",
    "            print(f\"Recycle cred.: {rec_b:,.2f} →  {rec_a:,.2f}  (Δ {rec_a-rec_b:+,.2f})\")\n",
    "            print(f\"ΔFG (tph)    : {dfg_b:.6f} → {dfg_a:.6f} (Δ {dfg_a-dfg_b:+.6f}) × Fuel Gas={fg_price:,.2f} → FG cost Δ: {(fgc_a-fgc_b):+,.2f}\")\n",
    "            print(f\"MARGIN       : {m_b:,.2f} → {m_a:,.2f} (Δ {m_a-m_b:+,.2f})\")\n",
    "\n",
    "            (OUT_DIR / \"audits\").mkdir(exist_ok=True, parents=True)\n",
    "            items.to_csv(OUT_DIR / \"audits\" / f\"audit_items_{ts:%Y%m%d_%H%M}.csv\", index=False)\n",
    "            summary = dict(\n",
    "                ts=str(ts),\n",
    "                prices=dict(canonical=pr_c, feeds=pr_f),\n",
    "                revenue_before=rev_b, revenue_after=rev_a,\n",
    "                feed_before=feed_b, feed_after=feed_a,\n",
    "                recycle_before=rec_b, recycle_after=rec_a,\n",
    "                dfg_tph_before=dfg_b, dfg_tph_after=dfg_a, fuelgas_price=fg_price,\n",
    "                fg_cost_before=fgc_b, fg_cost_after=fgc_a,\n",
    "                margin_before=m_b, margin_after=m_a,\n",
    "                rcot_opt=res.get('rcot_opt',{})\n",
    "            )\n",
    "            (OUT_DIR / \"audits\" / f\"audit_summary_{ts:%Y%m%d_%H%M}.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "            # Save curves (overrides-only α; rc0-injected)\n",
    "            try:\n",
    "                setter = {'LF_NAPH': gpmod.rcot_setter_lf_naph,\n",
    "                          'GF_GAS':  gpmod.rcot_setter_gf_gas,\n",
    "                          'GF_HYB_NAPH': gpmod.rcot_setter_hybrid}[geom]\n",
    "                rc_lo, rc_hi = RC_BOUNDS.get(geom, (810, 853))\n",
    "                rc_grid = _rc_grid_for(row_current, geom, rc_lo, rc_hi, points=15)\n",
    "                curve, x_row = gpmod.anchored_curve_at_ts(\n",
    "                    gp=gp, X_12h=X_12h, merged_lims=merged_lims, pipeline=pipeline,\n",
    "                    ml=ml_cached, ts=ts, rcot_setter=setter, rc_grid=rc_grid,\n",
    "                    use_gp_delta=True, alpha=0.0\n",
    "                )\n",
    "                (OUT_DIR / \"curves\").mkdir(exist_ok=True, parents=True)\n",
    "                curve.to_csv(OUT_DIR / \"curves\" / f\"curve_{ts:%Y%m%d_%H%M}_{geom}.csv\", index=False)\n",
    "                # slope fidelity at save time\n",
    "                rows = []\n",
    "                for p in gpmod.PRODUCTS:\n",
    "                    sc, cov = _robust_slope_metrics(curve, p)\n",
    "                    a_col, c_col = f'{p}_ANCHOR_tph', f'{p}_CORR_tph'\n",
    "                    diff = (curve[c_col].astype(float) - curve[a_col].astype(float)).abs().to_numpy(float) if (a_col in curve and c_col in curve) else np.array([np.nan])\n",
    "                    rows.append({'timestamp': ts, 'geometry': geom, 'product': p,\n",
    "                                 'slope_corr': sc, 'sign_cov': cov, 'anchor_miss_min': float(np.nanmin(diff))})\n",
    "                pd.DataFrame(rows).to_csv(OUT_DIR / \"curves\" / f\"audit_{ts:%Y%m%d_%H%M}_{geom}.csv\", index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Curve/audit save failed at {ts}: {e}\")\n",
    "\n",
    "            # persist recommendations row\n",
    "            rec_rows.append(flat)\n",
    "\n",
    "            # ---------- CLOSED-LOOP: apply setpoints & update simulated lags ----------\n",
    "            if mode == 'closed_loop':\n",
    "                knobs_applied = {k: float(v) for k, v in res['rcot_opt'].items()}\n",
    "\n",
    "                if apply_timing == 'next_stamp':\n",
    "                    idx = X_state.index\n",
    "                    next_mask = idx > ts\n",
    "                    if next_mask.any():\n",
    "                        t1 = idx[next_mask][0]\n",
    "                        for knob, val in knobs_applied.items():\n",
    "                            if knob in X_state.columns:\n",
    "                                X_state.loc[idx >= t1, knob] = val\n",
    "                        print(f\"[APPLIED next_stamp] {ts} -> {t1}\")\n",
    "                        klist = list(knobs_applied.keys())\n",
    "                        print(\"  rcots@ts   :\", X_state.loc[ts,  klist].to_dict())\n",
    "                        print(\"  rcots@next :\", X_state.loc[t1, klist].to_dict())\n",
    "                else:\n",
    "                    next_day = ts.normalize() + pd.Timedelta(days=1)\n",
    "                    for knob, val in knobs_applied.items():\n",
    "                        if knob in X_state.columns:\n",
    "                            X_state.loc[X_state.index >= next_day, knob] = val\n",
    "                    print(f\"[SCHEDULED next_day] {ts} -> {next_day}: {knobs_applied}\")\n",
    "\n",
    "                # write simulated corrected yields for this stamp to Y_sim_state (for ML lags)\n",
    "                if Y_sim_state is not None:\n",
    "                    for c in TARGET_COLS:\n",
    "                        if c in Y_sim_state.columns:\n",
    "                            Y_sim_state.at[ts, c] = float(res['yields_opt'].get(c, np.nan))\n",
    "\n",
    "        # append day’s rows\n",
    "        if rec_rows:\n",
    "            df_new = pd.DataFrame(rec_rows)\n",
    "            _safe_merge_csv(OUT_DIR / f\"rcot_recommendations{cache_tag}.csv\", df_new, key='timestamp')\n",
    "\n",
    "    # --------- END OF RUN: counterfactual check ---------\n",
    "    rec_path = OUT_DIR / f\"rcot_recommendations{cache_tag}.csv\"\n",
    "    if rec_path.exists():\n",
    "        recs = pd.read_csv(rec_path, parse_dates=['timestamp']).sort_values('timestamp')\n",
    "        schedule = build_rcot_schedule_from_recs(recs)\n",
    "        X_sim = apply_schedule_to_X(X_12h, schedule, start=start, end=end, hold=\"hold_until_next\")\n",
    "        Y_sim, M_sim = simulate_path_corrected(\n",
    "            X_sim=X_sim, merged_lims=merged_lims, pipeline=pipeline,\n",
    "            gp=gp, gps_dict=gps_dict, feature_cols_gp=feature_cols_gp,\n",
    "            price_provider=pp, total_spyro_yield_for_now=total_spyro_yield_for_now,\n",
    "            fg_consts=fg_consts, alpha_overrides=None,\n",
    "            start=start, end=end\n",
    "        )\n",
    "        (OUT_DIR / \"counterfactual\").mkdir(exist_ok=True, parents=True)\n",
    "        X_sim.to_csv(OUT_DIR / \"counterfactual\" / \"X_sim_rcot_applied.csv\")\n",
    "        Y_sim.to_csv(OUT_DIR / \"counterfactual\" / \"Y_sim_corrected.csv\")\n",
    "        M_sim.to_csv(OUT_DIR / \"counterfactual\" / \"M_sim_margin.csv\")\n",
    "\n",
    "        if 'margin_baseline_per_h' in recs.columns:\n",
    "            compare = (pd.DataFrame({'margin_hist': recs.set_index('timestamp')['margin_baseline_per_h']})\n",
    "                        .join(M_sim.rename(columns={'margin_per_h':'margin_sim'}), how='inner'))\n",
    "            print(\"\\n--- Margin hist vs sim (first 6 rows) ---\")\n",
    "            print(compare.head(6).to_string())\n",
    "\n",
    "    print(\"✅ Production run complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac6b0b7",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cda512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1376, 100) (1376, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\EFOM\\src\\data_loading.py:425: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['month'] = df['timestamp'].dt.to_period('M').dt.to_timestamp()\n"
     ]
    }
   ],
   "source": [
    "import src.data_loading as dl; reload(dl)\n",
    "from src.data_loading import DataPaths, ResampleConfig, DataPipeline\n",
    "\n",
    "paths = DataPaths(\n",
    "    input_dir=Path(\"input\"), inter_dir=Path(\"intermediate\"),\n",
    "    prod_excel=\"1. 생산량 Data_'23.07~'25.05_R1_송부용.xlsx\",\n",
    "    furn_excel=\"2. Furnace Data_'23.07~'25.05_R0.xlsx\",\n",
    "    nap_excel =\"Nap Feed 조성분석값.xlsx\",\n",
    "    gas_excel =\"Gas Feed 조성분석값.xlsx\",\n",
    "    recycle_excel=\"6. 에탄 및 프로판 데이터.xlsx\",\n",
    "    # cost_excel=\"마진가격_vModel_v250625.xlsx\",\n",
    "    price_csv= \"price.csv\",\n",
    "    util_excel=\"#1ECU 유틸리티사용량일별데이터.xlsx\",\n",
    "    fresh_excel=\"7. Gas Furnace Feed Data_'23.07~'25.05_r2.xlsx\",\n",
    "\n",
    "    # PKL caches (optional)\n",
    "    prod_pkl=\"df_production_v4.pkl\", furn_pkl=\"furnace.pkl\",\n",
    "    nap_pkl =\"df_feed_naptha.pkl\", gas_pkl =\"df_feed_gas.pkl\",\n",
    "    fresh_pkl= 'df_feed_fresh_v3.pkl', rec_pkl =\"df_recycle.pkl\",\n",
    "    prod_header=2, furn_header=2, nap_header=1, gas_header=1, rec_header=4, fresh_header=3\n",
    ")\n",
    "cfg = ResampleConfig(hour_freq='h', win12_freq='12h', win12_offset='9h')\n",
    "\n",
    "\n",
    "feature_rename = {\n",
    "    # map your util feature names → canonical (only if you use util models)\n",
    "    'Naph': 'Naphtha_chamber1', 'T-DAO': 'T-DAO_chamber1', 'DS': 'DS_chamber1',\n",
    "    'RCOT Ave.': 'RCOT_chamber1', 'Excess O2': \"Excess O2_chamber1\",\n",
    "    'Naph.1': 'Naphtha_chamber2', 'T-DAO.1': 'T-DAO_chamber2','DS.1': 'DS_chamber2',\n",
    "    'RCOT Ave..1': 'RCOT_chamber2', 'Excess O2.1': \"Excess O2_chamber2\",\n",
    "    'Naph.2': 'Naphtha_chamber3', 'T-DAO.2': 'T-DAO_chamber3','DS.2': 'DS_chamber3',\n",
    "    'RCOT Ave..2': 'RCOT_chamber3', 'Excess O2.2': \"Excess O2_chamber3\",\n",
    "    'Naph.3': 'Naphtha_chamber4', 'GAS': 'Gas Feed_chamber4','DS.3': 'DS_chamber4',\n",
    "    'RCOT Ave..3': 'RCOT_chamber4', 'Excess O2.3': \"Excess O2_chamber4\",\n",
    "    'Naph.4': 'Naphtha_chamber5', 'GAS.1': 'Gas Feed_chamber5','DS.4': 'DS_chamber5',\n",
    "    'RCOT Ave..4': 'RCOT_chamber5', 'Excess O2.4': \"Excess O2_chamber5\",\n",
    "    'Naph.5': 'Naphtha_chamber6', 'GAS.2': 'Gas Feed_chamber6','DS.5': 'DS_chamber6',\n",
    "    'RCOT Ave..5': 'RCOT_chamber6', 'Excess O2.5': \"Excess O2_chamber6\",\n",
    "}\n",
    "target_rename  = { 'Unnamed: 36':'steam','ECU F/G':'fuel_gas','ECU Elec..1':'electricity' }\n",
    "\n",
    "dp = DataPipeline(paths, cfg).run(feature_rename, target_rename)\n",
    "art = dp.artifacts()\n",
    "X_12h, Y_12h, util_df, prices_df = art['X_12h'], art['Y_12h'], art['util_df'], art['price_df']\n",
    "\n",
    "# clamp horizon (LIMS)\n",
    "X_12h = X_12h.loc[:END]\n",
    "Y_12h = Y_12h.loc[:END]\n",
    "print(X_12h.shape, Y_12h.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d1a76b",
   "metadata": {},
   "source": [
    "## 2) LIMS + SRTO pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53b8f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Downloads\\EFOM\\src\\data_loading.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  feed_gas['date'] = pd.to_datetime(feed_gas['date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "from src.data_loading import load_feed_data\n",
    "\n",
    "merged_lims = load_feed_data(\n",
    "    nap_path=paths.input_dir / \"복사본 (2024-25) ECU 투입 납사 세부성상-wt%.xlsx\",\n",
    "    gas_path=paths.input_dir / \"Gas Feed 조성분석값.xlsx\", header=1\n",
    ")\n",
    "merged_lims['date'] = pd.to_datetime(merged_lims['date'], errors='coerce')\n",
    "merged_lims = merged_lims.dropna(subset=['date']).sort_values('date')\n",
    "\n",
    "gas_cols = [c for c in ['Ethylene','Ethane','Propylene','Propane','n-Butane','i-Butane'] if c in merged_lims.columns]\n",
    "zr = (merged_lims[gas_cols].sum(axis=1) == 0)\n",
    "merged_lims.loc[zr, gas_cols] = np.nan\n",
    "merged_lims[gas_cols] = merged_lims[gas_cols].ffill().bfill()\n",
    "merged_lims = merged_lims.iloc[4:]  # keep (as you had)\n",
    "\n",
    "# SRTO DLL pipeline (plant spot)\n",
    "from src.srto_pipeline import SRTOConfig, RCOTSweepConfig, FeedConfig, SRTOPipeline\n",
    "from src.srto_components import component_index, MW\n",
    "dll_folder = Path(r\"C:\\Program Files\\Pyrotec\\SRTO\")\n",
    "selected_spy7 = [\n",
    "    dll_folder / \"01. GF_HYBRID MODE_SRTO7_NAPH.SPY7\",\n",
    "    dll_folder / \"04. LF_NAPH MODE_SRTO7.SPY7\",\n",
    "    dll_folder / \"07. GF_GAS MODE_SRTO7.SPY7\",\n",
    "]\n",
    "srto_config  = SRTOConfig(dll_folder, selected_spy7, component_index, MW)\n",
    "sweep_config = RCOTSweepConfig(rcot_min=790.0, rcot_max=900.0, rcot_step=2.0,\n",
    "                               chunk_size=10, n_jobs=6, save_checkpoints=True)\n",
    "feed_config  = FeedConfig(gas_components=gas_cols)\n",
    "pipeline     = SRTOPipeline(srto_config, sweep_config, feed_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b011b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_12h_with_lims shape: (1376, 110)\n",
      "Missing after merge: {'Paraffins': 368, 'Olefins': 368, 'Naphthenes': 368, 'Aromatics': 368, 'Ethylene': 368, 'Ethane': 368, 'Propylene': 368, 'Propane': 368, 'n-Butane': 368, 'i-Butane': 368}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5168\\562457764.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged[c] = merged[c].fillna(asof[c])\n"
     ]
    }
   ],
   "source": [
    "pona_cols = ['Paraffins','Olefins','Naphthenes','Aromatics']\n",
    "\n",
    "# Merge X_12h with merged_lims per AM/PM rule:\n",
    "# hour < 12 -> use previous day's LIMS, else use same day's LIMS; fallback to asof if day-join misses rows.\n",
    "x = X_12h.copy()\n",
    "ts = x.index.to_series()\n",
    "lims_date = ts.dt.normalize()\n",
    "lims_date = lims_date.where(ts.dt.hour >= 12, lims_date - pd.Timedelta(days=1))\n",
    "x = x.assign(lims_date=lims_date.values)\n",
    "\n",
    "m = merged_lims.copy()\n",
    "m['lims_date'] = pd.to_datetime(m['date'], errors='coerce').dt.normalize()\n",
    "m_daily = m.sort_values('date').groupby('lims_date', as_index=False).last()\n",
    "\n",
    "keep = pona_cols + gas_cols\n",
    "m_sel = m_daily[['lims_date'] + keep]\n",
    "\n",
    "xr = x.reset_index()\n",
    "idx_name = xr.columns[0]   # original timestamp column name\n",
    "merged = xr.merge(m_sel, on='lims_date', how='left').set_index(idx_name)\n",
    "merged.index.name = X_12h.index.name\n",
    "\n",
    "# fallback: fill any remaining NaNs with the most recent earlier merged_lims record\n",
    "if merged[keep].isna().any().any():\n",
    "    mr = m.sort_values('date')[['date'] + keep].rename(columns={'date': 'lims_ts'})\n",
    "    xr_ts = xr.copy(); xr_ts['ts'] = pd.to_datetime(xr_ts[idx_name])\n",
    "    asof = pd.merge_asof(xr_ts.sort_values('ts'),\n",
    "                         mr.sort_values('lims_ts'),\n",
    "                         left_on='ts', right_on='lims_ts', direction='backward').set_index(idx_name)\n",
    "    for c in keep:\n",
    "        merged[c] = merged[c].fillna(asof[c])\n",
    "\n",
    "X_12h_with_lims = X_12h.join(merged[keep])\n",
    "print('X_12h_with_lims shape:', X_12h_with_lims.shape)\n",
    "print('Missing after merge:', X_12h_with_lims[keep].isna().sum().to_dict())\n",
    "\n",
    "gas_rename = {c: f\"{c}_gas\" for c in gas_cols}  # e.g., 'Ethane' -> 'Ethane_gas'\n",
    "X_12h_with_lims = X_12h_with_lims.rename(columns=gas_rename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb073a90",
   "metadata": {},
   "source": [
    "## 3) Rolling \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb250d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- lightweight memo around total_spyro_yield_for_now ---\n",
    "class SpyroMemo:\n",
    "    def __init__(self, fn, key_cols=None, decimals=4, maxsize=200000):\n",
    "        self.fn = fn\n",
    "        self.key_cols = tuple(key_cols) if key_cols is not None else None  # ensure tuple\n",
    "        self.dec = decimals\n",
    "        self.cache = {}\n",
    "        self.maxsize = maxsize\n",
    "\n",
    "    def _select_cols(self, row: pd.Series):\n",
    "        if self.key_cols is None:\n",
    "            # stable, hashable selection\n",
    "            return tuple(\n",
    "                c for c in row.index\n",
    "                if c.startswith('RCOT')\n",
    "                or c.startswith('Naphtha_chamber')\n",
    "                or c.startswith('Gas Feed_chamber')\n",
    "            )\n",
    "        return self.key_cols\n",
    "\n",
    "    def _to_num(self, x):\n",
    "        try:\n",
    "            v = float(x)\n",
    "        except Exception:\n",
    "            v = 0.0\n",
    "        # handle NaN\n",
    "        if v != v:  # NaN check without importing math\n",
    "            v = 0.0\n",
    "        return round(v, self.dec)\n",
    "\n",
    "    def _sig(self, row: pd.Series, short_key: str):\n",
    "        cols = self._select_cols(row)                # tuple, hashable\n",
    "        vals = tuple(self._to_num(row.get(c, 0.0)) for c in cols)\n",
    "        return (short_key, cols, vals)               # fully hashable\n",
    "\n",
    "    def __call__(self, row: pd.Series, short_key: str, ctx=None):\n",
    "        k = self._sig(row, short_key)\n",
    "        v = self.cache.get(k)\n",
    "        if v is not None:\n",
    "            return v\n",
    "        v = self.fn(row, short_key, ctx=ctx)\n",
    "        if len(self.cache) < self.maxsize:\n",
    "            self.cache[k] = v\n",
    "        return v\n",
    "\n",
    "# wrap it\n",
    "# total_spyro_yield_for_now = SpyroMemo(total_spyro_yield_for_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d138d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCOT groups so setters touch the right columns\n",
    "gpmod.set_rcot_groups_from_columns(X_12h.columns)\n",
    "\n",
    "# Prices\n",
    "pp = opt.PriceProvider(prices_df)\n",
    "\n",
    "# If you haven't defined this yet, define a minimal SPYRO wrapper (ctx not required)\n",
    "_SHORT_TO_SRTO = {\n",
    "    'Ethylene':'Ethylene','Propylene':'Propylene','MixedC4':'MixedC4','RPG':'RPG',\n",
    "    'Ethane':'Ethane','Propane':'Propane',\n",
    "    'Fuel_Gas':'Fuel_Gas','Fuel Gas':'Fuel_Gas','FG':'Fuel_Gas','FuelGas':'Fuel_Gas',\n",
    "    'Tail Gas':'Tail_Gas', 'Tail_Gas' :'Tail_Gas'\n",
    "}\n",
    "def total_spyro_yield_for_now(row_like: pd.Series, short_key: str, ctx=None) -> float:\n",
    "    ts = getattr(row_like, 'name', None)\n",
    "    if ts is None: return 0.0\n",
    "    comp_row = merged_lims.loc[merged_lims['date'] <= ts].iloc[-1]\n",
    "    spot = pipeline.predict_spot_plant(row_like, comp_row, feed_thr=0.1)\n",
    "    if spot.get('status') != 'ok': return 0.0\n",
    "    key = _SHORT_TO_SRTO.get(short_key, short_key)\n",
    "    return float(spot['totals_tph'].get(key, 0.0))\n",
    "\n",
    "# Excel-delta margin wrapper (uses opt.delta_fg_excel by default)\n",
    "margin_excel = opt.make_margin_fn_excel_delta(\n",
    "    price_provider=pp,\n",
    "    total_spyro_yield_for_now=total_spyro_yield_for_now,\n",
    "    spyro_ctx=None,                                  # SPYRO_CTX can be None\n",
    "    fg_constants=opt.FuelGasConstants())\n",
    "\n",
    "# Bounds by geometry\n",
    "RC_BOUNDS = {\n",
    "    'LF_NAPH': (830.0, 853.0),\n",
    "    'GF_GAS':  (860.0, 890.0),\n",
    "    'GF_HYB_NAPH': (830.0, 853.0),\n",
    "}\n",
    "\n",
    "\n",
    "# wrap it\n",
    "memo_spyro = SpyroMemo(total_spyro_yield_for_now)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d26096c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML/GP window: 2024-07-05 09:00:00 → 2025-01-01 09:00:00\n",
      "Rows in lookback window: 360 (MIN_TR_ROWS = 180 )\n"
     ]
    }
   ],
   "source": [
    "first_eval_day = pd.Timestamp('2025-01-01')\n",
    "train_end = (X_12h.index[(X_12h.index >= first_eval_day) &\n",
    "                         (X_12h.index < first_eval_day + pd.Timedelta(days=1))][-1]\n",
    "             - pd.Timedelta(hours=12))\n",
    "train_start = train_end - LOOKBACK_6M\n",
    "print(\"ML/GP window:\", train_start, \"→\", train_end)\n",
    "\n",
    "window_rows = ((X_12h.index >= train_start) & (X_12h.index < train_end)).sum()\n",
    "print(\"Rows in lookback window:\", window_rows, \"(MIN_TR_ROWS =\", MIN_TR_ROWS, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceaa0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbed02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_prediction_check(\n",
    "    X_12h: pd.DataFrame,\n",
    "    Y_12h: pd.DataFrame,\n",
    "    start: str | pd.Timestamp,\n",
    "    end: str | pd.Timestamp,\n",
    "    *,\n",
    "    lookback: pd.Timedelta = pd.Timedelta(days=180),\n",
    "    min_tr_rows: int = MIN_TR_ROWS,\n",
    "    target_cols: Sequence[str] | None = None,\n",
    "    lgbm_params: dict | None = None,\n",
    "    Y_for_lags: pd.DataFrame | None = None,   # defaults to Y_12h (no leakage: uses only t-1)\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Rolling-window ML backtest with no cache. Fits a fresh model at each stamp\n",
    "    using [ts - lookback, ts) and predicts targets at ts. Reports R² and MAPE.\n",
    "\n",
    "    Returns:\n",
    "        preds_df: index=timestamp, columns=target_cols (predictions)\n",
    "        metrics_df: rows per target with r2, mape(%), n\n",
    "    \"\"\"\n",
    "    start = pd.Timestamp(start); end = pd.Timestamp(end)\n",
    "    idx = X_12h.index.sort_values()\n",
    "    stamps = idx[(idx >= start) & (idx <= end)]\n",
    "\n",
    "    # choose targets\n",
    "    if target_cols is None:\n",
    "        target_cols = effective_target_cols(Y_12h)\n",
    "    tcols = [c for c in target_cols if c in Y_12h.columns]\n",
    "    if not tcols:\n",
    "        raise ValueError(\"No matching target columns found in Y_12h.\")\n",
    "\n",
    "    # lags source\n",
    "    if Y_for_lags is None:\n",
    "        Y_for_lags = Y_12h\n",
    "\n",
    "    # metrics helpers\n",
    "    def _r2(y, yhat):\n",
    "        y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
    "        mask = np.isfinite(y) & np.isfinite(yhat)\n",
    "        y = y[mask]; yhat = yhat[mask]\n",
    "        if y.size < 2 or np.allclose(y, y.mean()):\n",
    "            return np.nan\n",
    "        ss_res = np.sum((y - yhat)**2)\n",
    "        ss_tot = np.sum((y - y.mean())**2)\n",
    "        return 1.0 - ss_res/ss_tot\n",
    "\n",
    "    def _mape(y, yhat, eps=1e-6):\n",
    "        y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
    "        mask = np.isfinite(y) & np.isfinite(yhat) & (np.abs(y) > eps)\n",
    "        if not np.any(mask):\n",
    "            return np.nan\n",
    "        return float(np.mean(np.abs((y[mask] - yhat[mask]) / y[mask])) * 100.0)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for ts in stamps:\n",
    "        ts_start = ts - lookback\n",
    "        tr_idx = idx[(idx >= ts_start) & (idx < ts)]\n",
    "        if len(tr_idx) < min_tr_rows:\n",
    "            # not enough history; record NaNs\n",
    "            rows.append({\"timestamp\": ts, **{c: np.nan for c in tcols}})\n",
    "            continue\n",
    "\n",
    "        ml = MLPredictor(\n",
    "            target_cols=tcols,\n",
    "            cfg=MLPredictorConfig(\n",
    "                ds_prefixes=[\"DS_chamber\"],\n",
    "                add_virtual_rcots=True,\n",
    "                build_lag1_from_targets=True,\n",
    "                lgbm_params=(lgbm_params or dict(verbosity=-1, n_jobs=2)),\n",
    "            ),\n",
    "        ).fit(X_12h.loc[tr_idx], Y_12h.loc[tr_idx])\n",
    "\n",
    "        pred = ml.predict_row(X_12h.loc[ts], Y_for_lags=Y_for_lags)\n",
    "        rows.append({\"timestamp\": ts, **{c: float(pred.get(c, np.nan)) for c in tcols}})\n",
    "\n",
    "    preds_df = pd.DataFrame(rows).set_index(\"timestamp\").sort_index()\n",
    "\n",
    "    # compute metrics against ground truth at the same stamps\n",
    "    metrics = []\n",
    "    for c in tcols:\n",
    "        y_true = Y_12h.loc[preds_df.index, c]\n",
    "        y_pred = preds_df[c]\n",
    "        metrics.append({\n",
    "            \"target\": c,\n",
    "            \"r2\": _r2(y_true, y_pred),\n",
    "            \"mape_pct\": _mape(y_true, y_pred),\n",
    "            \"n\": int(np.isfinite(y_true).sum())\n",
    "        })\n",
    "    metrics_df = pd.DataFrame(metrics).sort_values(\"target\").reset_index(drop=True)\n",
    "\n",
    "    # optional overall row\n",
    "    if not metrics_df.empty:\n",
    "        metrics_df = pd.concat([\n",
    "            metrics_df,\n",
    "            pd.DataFrame([{\n",
    "                \"target\": \"_overall_mean\",\n",
    "                \"r2\": float(np.nanmean(metrics_df[\"r2\"])),\n",
    "                \"mape_pct\": float(np.nanmean(metrics_df[\"mape_pct\"])),\n",
    "                \"n\": int(np.sum(metrics_df[\"n\"]))\n",
    "            }])\n",
    "        ], ignore_index=True)\n",
    "\n",
    "    return preds_df, metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a684a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               target        r2  mape_pct     n\n",
      "0     Ethane_prod_t+1  0.772525  1.554266   276\n",
      "1   Ethylene_prod_t+1  0.849902  1.484865   276\n",
      "2   Hydrogen_prod_t+1  0.880489  1.472727   276\n",
      "3    MixedC4_prod_t+1  0.962875  1.335622   276\n",
      "4    Propane_prod_t+1  0.871572  6.083350   276\n",
      "5  Propylene_prod_t+1  0.880260  1.626113   276\n",
      "6        RPG_prod_t+1  0.831349  4.046858   276\n",
      "7   Tail_Gas_prod_t+1  0.760107  1.687344   276\n",
      "8       _overall_mean  0.851135  2.411393  2208\n"
     ]
    }
   ],
   "source": [
    "preds, metrics = ml_prediction_check(\n",
    "    X_12h=X_12h,\n",
    "    Y_12h=Y_12h,\n",
    "    start=\"2025-01-01\",\n",
    "    end=\"2025-05-19\",\n",
    "    lookback=pd.Timedelta(\"180D\"),\n",
    "    target_cols=TARGET_COLS,\n",
    "    min_tr_rows=MIN_TR_ROWS,\n",
    ")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcac3882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ethylene_prod_t+1</th>\n",
       "      <th>Propylene_prod_t+1</th>\n",
       "      <th>MixedC4_prod_t+1</th>\n",
       "      <th>RPG_prod_t+1</th>\n",
       "      <th>Ethane_prod_t+1</th>\n",
       "      <th>Propane_prod_t+1</th>\n",
       "      <th>Hydrogen_prod_t+1</th>\n",
       "      <th>Tail_Gas_prod_t+1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-01 09:00:00</th>\n",
       "      <td>64.632181</td>\n",
       "      <td>35.820969</td>\n",
       "      <td>23.833408</td>\n",
       "      <td>53.810319</td>\n",
       "      <td>20.238578</td>\n",
       "      <td>4.194660</td>\n",
       "      <td>1.679983</td>\n",
       "      <td>31.521825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-01 21:00:00</th>\n",
       "      <td>64.882430</td>\n",
       "      <td>35.175941</td>\n",
       "      <td>23.538400</td>\n",
       "      <td>54.114994</td>\n",
       "      <td>20.158146</td>\n",
       "      <td>4.390323</td>\n",
       "      <td>1.666618</td>\n",
       "      <td>31.725918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 09:00:00</th>\n",
       "      <td>64.861108</td>\n",
       "      <td>34.805212</td>\n",
       "      <td>23.790432</td>\n",
       "      <td>53.884320</td>\n",
       "      <td>20.219559</td>\n",
       "      <td>4.187359</td>\n",
       "      <td>1.675382</td>\n",
       "      <td>31.785543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02 21:00:00</th>\n",
       "      <td>64.660617</td>\n",
       "      <td>34.897149</td>\n",
       "      <td>24.383160</td>\n",
       "      <td>52.100815</td>\n",
       "      <td>20.777666</td>\n",
       "      <td>4.160093</td>\n",
       "      <td>1.668695</td>\n",
       "      <td>31.367354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-03 09:00:00</th>\n",
       "      <td>65.236371</td>\n",
       "      <td>35.398415</td>\n",
       "      <td>24.618357</td>\n",
       "      <td>53.054653</td>\n",
       "      <td>20.761556</td>\n",
       "      <td>5.257886</td>\n",
       "      <td>1.684727</td>\n",
       "      <td>31.340062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-16 21:00:00</th>\n",
       "      <td>71.094542</td>\n",
       "      <td>36.350091</td>\n",
       "      <td>25.852034</td>\n",
       "      <td>49.092301</td>\n",
       "      <td>18.971693</td>\n",
       "      <td>3.238639</td>\n",
       "      <td>2.151448</td>\n",
       "      <td>31.790431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-17 09:00:00</th>\n",
       "      <td>71.861226</td>\n",
       "      <td>36.568034</td>\n",
       "      <td>25.710092</td>\n",
       "      <td>48.058389</td>\n",
       "      <td>19.503060</td>\n",
       "      <td>3.149995</td>\n",
       "      <td>2.156298</td>\n",
       "      <td>31.721379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-17 21:00:00</th>\n",
       "      <td>71.115500</td>\n",
       "      <td>36.402894</td>\n",
       "      <td>25.546277</td>\n",
       "      <td>45.900704</td>\n",
       "      <td>19.170134</td>\n",
       "      <td>3.189867</td>\n",
       "      <td>2.155668</td>\n",
       "      <td>31.603605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-18 09:00:00</th>\n",
       "      <td>69.913986</td>\n",
       "      <td>36.182660</td>\n",
       "      <td>25.885385</td>\n",
       "      <td>47.660974</td>\n",
       "      <td>19.302983</td>\n",
       "      <td>3.017533</td>\n",
       "      <td>2.134010</td>\n",
       "      <td>31.904289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-18 21:00:00</th>\n",
       "      <td>70.040379</td>\n",
       "      <td>37.105023</td>\n",
       "      <td>26.067312</td>\n",
       "      <td>47.265253</td>\n",
       "      <td>19.164641</td>\n",
       "      <td>3.084999</td>\n",
       "      <td>2.105516</td>\n",
       "      <td>33.249765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Ethylene_prod_t+1  Propylene_prod_t+1  MixedC4_prod_t+1  \\\n",
       "timestamp                                                                      \n",
       "2025-01-01 09:00:00          64.632181           35.820969         23.833408   \n",
       "2025-01-01 21:00:00          64.882430           35.175941         23.538400   \n",
       "2025-01-02 09:00:00          64.861108           34.805212         23.790432   \n",
       "2025-01-02 21:00:00          64.660617           34.897149         24.383160   \n",
       "2025-01-03 09:00:00          65.236371           35.398415         24.618357   \n",
       "...                                ...                 ...               ...   \n",
       "2025-05-16 21:00:00          71.094542           36.350091         25.852034   \n",
       "2025-05-17 09:00:00          71.861226           36.568034         25.710092   \n",
       "2025-05-17 21:00:00          71.115500           36.402894         25.546277   \n",
       "2025-05-18 09:00:00          69.913986           36.182660         25.885385   \n",
       "2025-05-18 21:00:00          70.040379           37.105023         26.067312   \n",
       "\n",
       "                     RPG_prod_t+1  Ethane_prod_t+1  Propane_prod_t+1  \\\n",
       "timestamp                                                              \n",
       "2025-01-01 09:00:00     53.810319        20.238578          4.194660   \n",
       "2025-01-01 21:00:00     54.114994        20.158146          4.390323   \n",
       "2025-01-02 09:00:00     53.884320        20.219559          4.187359   \n",
       "2025-01-02 21:00:00     52.100815        20.777666          4.160093   \n",
       "2025-01-03 09:00:00     53.054653        20.761556          5.257886   \n",
       "...                           ...              ...               ...   \n",
       "2025-05-16 21:00:00     49.092301        18.971693          3.238639   \n",
       "2025-05-17 09:00:00     48.058389        19.503060          3.149995   \n",
       "2025-05-17 21:00:00     45.900704        19.170134          3.189867   \n",
       "2025-05-18 09:00:00     47.660974        19.302983          3.017533   \n",
       "2025-05-18 21:00:00     47.265253        19.164641          3.084999   \n",
       "\n",
       "                     Hydrogen_prod_t+1  Tail_Gas_prod_t+1  \n",
       "timestamp                                                  \n",
       "2025-01-01 09:00:00           1.679983          31.521825  \n",
       "2025-01-01 21:00:00           1.666618          31.725918  \n",
       "2025-01-02 09:00:00           1.675382          31.785543  \n",
       "2025-01-02 21:00:00           1.668695          31.367354  \n",
       "2025-01-03 09:00:00           1.684727          31.340062  \n",
       "...                                ...                ...  \n",
       "2025-05-16 21:00:00           2.151448          31.790431  \n",
       "2025-05-17 09:00:00           2.156298          31.721379  \n",
       "2025-05-17 21:00:00           2.155668          31.603605  \n",
       "2025-05-18 09:00:00           2.134010          31.904289  \n",
       "2025-05-18 21:00:00           2.105516          33.249765  \n",
       "\n",
       "[276 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a008f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS_ML = TARGET_COLS\n",
    "\n",
    "# Example daily run for the whole horizon (or pass a shorter end date)\n",
    "run_production(\n",
    "    X_12h=X_12h, Y_12h=Y_12h,\n",
    "    merged_lims=merged_lims, pipeline=pipeline,\n",
    "    prices_df=prices_df,\n",
    "    total_spyro_yield_for_now=memo_spyro,\n",
    "    start=pd.Timestamp('2025-01-01'), end=pd.Timestamp('2025-05-19'),\n",
    "    mode='closed_loop',\n",
    "    closed_loop_opts=dict(\n",
    "        apply_timing='next_stamp',          # or 'next_stamp'\n",
    "        hold_policy='hold_until_next',    # step & hold\n",
    "        ml_train_mode='simulated',       # train ML on historical windows (fast/stable)\n",
    "        gp_train_mode='simulated',       # train GP on historical windows\n",
    "        cache_tag='_sim'                  # keep caches separate\n",
    "    )\n",
    ")                                                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
